{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Project: Making a prediction of the Programming Language base on README.md\n",
    "\"We don't know, what we don't know\"  \n",
    "\n",
    "By: Cody Watson and Eric Escalante  \n",
    "May 13, 2019  \n",
    "\n",
    "In this Jupyter Notebook, we will be scraping data from GitHub repository README files. The goal is to build a model that can predict which programming language a repository is using, given the text of the README file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "**Import the necessary packages and their use cases for this project:**\n",
    "> **pandas:** data frames and data manipulation  \n",
    "> **numpy:** summary statistics  \n",
    "> **matplotlib:** used for visualizations  \n",
    "> **seasborn:** fancy visualizations  \n",
    "> **datetime:** turn the dates into datetime objects / get day of week  \n",
    "> **warning:** used to ignore python warnings  \n",
    "> **requests:** to obtain the HTML from the page  \n",
    "> **unicodedata:** character encoding  \n",
    "> **BeautifulSoup:** to parse the HTML and obtain the text/data that we want  \n",
    "> **nltk:**  \n",
    "> **WordCloud:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import NLP_acquire\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "1. [Project Planning](#project-planning)\n",
    "1. [Preparation](#preparation)\n",
    "1. [Exploration](#exploration)\n",
    "1. [Modeling](#modeling)\n",
    "1. [Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Planning <a name=\"project-planning\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals  \n",
    "> Goals for the Project are:  \n",
    "1. Accurately predict the programming languages based on mutliple programming languages from Github README files\n",
    "2. Create different WordCloud models showing the most commonly used words with each programming languages\n",
    "3. Built muliple Classification machine learning models to accurately predict which language the repository is written in\n",
    "4. Be sure that we are documenting our thoughts throughout the process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverables\n",
    "> 1. A well-documented jupyter notebook that contains your analysis  \n",
    "> 2. One or two google slides suitable for a general audience that summarize your findings. Include a well-labelled visualization in your slides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypotheses\n",
    "> \"C++ programmers are more elitist\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts & Questions\n",
    "> **Thoughts:**  \n",
    "- Figure out how to apply multiple Classification methods to predict programming language using the repo's readme.md\n",
    "- Figure out how to apply multiple sentiment analysis methods to the data\n",
    "- Compare and Contrast different Corpus using TF-IDF\n",
    "- We want to learn how to set up the Word2Vec for word embedding\n",
    "- Apply a 'github' image mask to a WordCloud (within the negative/positive space; label each space with different colors)\n",
    "\n",
    "> **Questions:**  \n",
    "- What am I?\n",
    "- Does the sentiment in a given languange vier more towards positive or negative?\n",
    "- How many graphs can we make this weekend??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Environment <a name=\"preparation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bring in the data from the prepare file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('data.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Going to see what the top portion looks like**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>readme_contents</th>\n",
       "      <th>repo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JavaScript</td>\n",
       "      <td>&lt;p align=\"center\"&gt;\\n  &lt;a href=\"https://getboot...</td>\n",
       "      <td>twbs/bootstrap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JavaScript</td>\n",
       "      <td># [React](https://reactjs.org/) &amp;middot; [![Gi...</td>\n",
       "      <td>facebook/react</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>This page is available as an easy-to-read webs...</td>\n",
       "      <td>EbookFoundation/free-programming-books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;div align=\"center\"&gt;\\n\\t&lt;img width=\"500\" heigh...</td>\n",
       "      <td>sindresorhus/awesome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>![Web Developer Roadmap - 2019](https://i.imgu...</td>\n",
       "      <td>kamranahmedse/developer-roadmap</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     language                                    readme_contents  \\\n",
       "0  JavaScript  <p align=\"center\">\\n  <a href=\"https://getboot...   \n",
       "1  JavaScript  # [React](https://reactjs.org/) &middot; [![Gi...   \n",
       "2        None  This page is available as an easy-to-read webs...   \n",
       "3        None  <div align=\"center\">\\n\\t<img width=\"500\" heigh...   \n",
       "4        None  ![Web Developer Roadmap - 2019](https://i.imgu...   \n",
       "\n",
       "                                     repo  \n",
       "0                          twbs/bootstrap  \n",
       "1                          facebook/react  \n",
       "2  EbookFoundation/free-programming-books  \n",
       "3                    sindresorhus/awesome  \n",
       "4         kamranahmedse/developer-roadmap  "
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Funtions to clean our readme files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(text):\n",
    "    '''\n",
    "    Function that takes a string and normalized the text using unicodedata\n",
    "    '''\n",
    "    text = unicodedata.normalize('NFKD', text.lower())\\\n",
    "        .encode('ascii', 'ignore')\\\n",
    "        .decode('utf-8', 'ignore')\n",
    "    return re.sub(r\"[^a-z0-9'\\s]\", '', text)\n",
    "\n",
    "def lemmatize(text):\n",
    "    '''\n",
    "    Function that lemmatizes the string\n",
    "    '''\n",
    "    nlp = spacy.load('en', parse=True, tag=True, entity=True)\n",
    "    doc = nlp(text) # process the text with spacy\n",
    "    lemmas = [word.lemma_ for word in doc]\n",
    "    text_lemmatized = ' '.join(lemmas)\n",
    "    return re.sub(r\"\\s*(-PRON-|\\'s|\\')\", '', text_lemmatized)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    '''\n",
    "    Function to remove stopwords from the string\n",
    "    '''\n",
    "    tokenizer = ToktokTokenizer()\n",
    "    stopword_list = stopwords.words('english')\n",
    "    stopword_list.remove('no')\n",
    "    stopword_list.remove('not')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    filtered_tokens = [t for t in tokens if t not in stopword_list]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "def clean_readme(string):\n",
    "    return remove_stopwords(lemmatize(basic_clean(string)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply our funcitons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['readme_clean'] = df.readme_contents.apply(clean_readme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thoughts:**\n",
    "> 1. We no longer need the original readme contents, going to dump them\n",
    "> 2. We need to now group by the language\n",
    "> 3. We are going to start using WordCloud to visually see the frequencies of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='readme_contents', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration  <a name=\"exploration\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thoughts:**\n",
    "> 1. Look at Top\\Bottom 10 words in each readme grouped by programming language\n",
    "> 2. Sentiment Analysis on each languange\n",
    "> 3. Figure out how to apply latent Dirichlet allocation\n",
    "> 4. Learn how to set up Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling <a name=\"modeling\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bring in multiple classification models**\n",
    "> _ToDo_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split\n",
    "> _ToDo_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize Conclusions <a name=\"summary\"></a>\n",
    "> _ToDo_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find different ways to improve model:\n",
    "> _ToDo_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

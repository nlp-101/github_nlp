{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Project: Making a prediction of the Programming Language base on README.md\n",
    "\"We don't know, what we don't know\"  \n",
    "\n",
    "By: Cody Watson and Eric Escalante  \n",
    "May 13, 2019  \n",
    "\n",
    "In this Jupyter Notebook, we will be scraping data from GitHub repository README files. The goal is to build a model that can predict which programming language a repository is using, given the text of the README file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "**Import the necessary packages and their use cases for this project:**\n",
    "> **pandas:** data frames and data manipulation  \n",
    "> **numpy:** summary statistics  \n",
    "> **matplotlib:** used for visualizations  \n",
    "> **seasborn:** fancy visualizations  \n",
    "> **datetime:** turn the dates into datetime objects / get day of week  \n",
    "> **warning:** used to ignore python warnings  \n",
    "> **requests:** to obtain the HTML from the page  \n",
    "> **unicodedata:** character encoding  \n",
    "> **BeautifulSoup:** to parse the HTML and obtain the text/data that we want  \n",
    "> **nltk:**  \n",
    "> **WordCloud:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import NLP_acquire\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "1. [Project Planning](#project-planning)\n",
    "1. [Preparation](#preparation)\n",
    "1. [Exploration](#exploration)\n",
    "1. [Modeling](#modeling)\n",
    "1. [Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Planning <a name=\"project-planning\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals  \n",
    "> Goals for the Project are:  \n",
    "1. Accurately predict the programming languages based on mutliple programming languages from Github README files\n",
    "2. Create different WordCloud models showing the most commonly used words with each programming languages\n",
    "3. Built muliple Classification machine learning models to accurately predict which language the repository is written in\n",
    "4. Be sure that we are documenting our thoughts throughout the process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverables\n",
    "> 1. A well-documented jupyter notebook that contains your analysis  \n",
    "> 2. One or two google slides suitable for a general audience that summarize your findings. Include a well-labelled visualization in your slides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypotheses\n",
    "> _ToDo_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts & Questions\n",
    "> **Thoughts:**  \n",
    "_ToDo_\n",
    "\n",
    "> **Questions:**  \n",
    "_ToDo_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Environment <a name=\"preparation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bring in the data from the prepare file**\n",
    "> _ToDo_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>readme_contents</th>\n",
       "      <th>repo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JavaScript</td>\n",
       "      <td>&lt;p align=\"center\"&gt;\\n  &lt;a href=\"https://getboot...</td>\n",
       "      <td>twbs/bootstrap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JavaScript</td>\n",
       "      <td># [React](https://reactjs.org/) &amp;middot; [![Gi...</td>\n",
       "      <td>facebook/react</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>This page is available as an easy-to-read webs...</td>\n",
       "      <td>EbookFoundation/free-programming-books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;div align=\"center\"&gt;\\n\\t&lt;img width=\"500\" heigh...</td>\n",
       "      <td>sindresorhus/awesome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>![Web Developer Roadmap - 2019](https://i.imgu...</td>\n",
       "      <td>kamranahmedse/developer-roadmap</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     language                                    readme_contents  \\\n",
       "0  JavaScript  <p align=\"center\">\\n  <a href=\"https://getboot...   \n",
       "1  JavaScript  # [React](https://reactjs.org/) &middot; [![Gi...   \n",
       "2        None  This page is available as an easy-to-read webs...   \n",
       "3        None  <div align=\"center\">\\n\\t<img width=\"500\" heigh...   \n",
       "4        None  ![Web Developer Roadmap - 2019](https://i.imgu...   \n",
       "\n",
       "                                     repo  \n",
       "0                          twbs/bootstrap  \n",
       "1                          facebook/react  \n",
       "2  EbookFoundation/free-programming-books  \n",
       "3                    sindresorhus/awesome  \n",
       "4         kamranahmedse/developer-roadmap  "
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(text):\n",
    "    text = unicodedata.normalize('NFKD', text.lower())\\\n",
    "        .encode('ascii', 'ignore')\\\n",
    "        .decode('utf-8', 'ignore')\n",
    "    return re.sub(r\"[^a-z0-9'\\s]\", '', text)\n",
    "\n",
    "def lemmatize(text):\n",
    "    nlp = spacy.load('en', parse=True, tag=True, entity=True)\n",
    "    doc = nlp(text) # process the text with spacy\n",
    "    lemmas = [word.lemma_ for word in doc]\n",
    "    text_lemmatized = ' '.join(lemmas)\n",
    "    return re.sub(r\"\\s*(-PRON-|\\'s|\\')\", '', text_lemmatized)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokenizer = ToktokTokenizer()\n",
    "    stopword_list = stopwords.words('english')\n",
    "    stopword_list.remove('no')\n",
    "    stopword_list.remove('not')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    filtered_tokens = [t for t in tokens if t not in stopword_list]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "def clean_readme(string):\n",
    "    return remove_stopwords(lemmatize(basic_clean(string)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['readme_clean'] = df.readme_contents.apply(clean_readme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='readme_contents', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration  <a name=\"exploration\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make some awesome graphs**\n",
    "> _ToDo_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling <a name=\"modeling\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bring in multiple classification models**\n",
    "> _ToDo_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split\n",
    "> _ToDo_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize Conclusions <a name=\"summary\"></a>\n",
    "> _ToDo_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find different ways to improve model:\n",
    "> _ToDo_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

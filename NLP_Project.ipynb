{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Project: Making a prediction of the Programming Language base on README.md\n",
    "\"We don't know, what we don't know\"  \n",
    "\n",
    "By: Cody Watson and Eric Escalante  \n",
    "May 13, 2019  \n",
    "\n",
    "In this Jupyter Notebook, we will be scraping data from GitHub repository README files. The goal is to build a model that can predict which programming language a repository is using, given the text of the README file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "**Import the necessary packages and their use cases for this project:**\n",
    "> **pandas:** data frames and data manipulation  \n",
    "> **numpy:** summary statistics  \n",
    "> **matplotlib:** used for visualizations  \n",
    "> **seasborn:** fancy visualizations  \n",
    "> **datetime:** turn the dates into datetime objects / get day of week  \n",
    "> **warning:** used to ignore python warnings  \n",
    "> **requests:** to obtain the HTML from the page  \n",
    "> **unicodedata:** character encoding  \n",
    "> **BeautifulSoup:** to parse the HTML and obtain the text/data that we want  \n",
    "> **nltk:** Natual Language Toolkit that allows us to work with human language data  \n",
    "> **WordCloud:** creates an image composed of words used in a particular text or subject, in which the size of each word indicates its frequency or importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import NLP_acquire\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "1. [Project Planning](#project-planning)\n",
    "1. [Preparation](#preparation)\n",
    "1. [Exploration](#exploration)\n",
    "1. [Modeling](#modeling)\n",
    "1. [Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Planning <a name=\"project-planning\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals  \n",
    "> Goals for the Project are:  \n",
    "1. Accurately predict the programming languages based on mutliple programming languages from Github README files\n",
    "2. Create different WordCloud models showing the most commonly used words with each programming languages\n",
    "3. Built muliple Classification machine learning models to accurately predict which language the repository is written in\n",
    "4. Be sure that we are documenting our thoughts throughout the process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverables\n",
    ">- A well-documented jupyter notebook that contains your analysis  \n",
    ">- One or two google slides suitable for a general audience that summarize your findings. Include a well-labelled visualization in your slides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypotheses\n",
    "> \"C++ programmers are more elitist\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts & Questions\n",
    "> **Thoughts:**  \n",
    "- Figure out how to apply multiple Classification methods to predict programming language using the repo's readme.md\n",
    "- Figure out how to apply multiple sentiment analysis methods to the data\n",
    "- Compare and Contrast different Corpus using TF-IDF\n",
    "- We want to learn how to set up the Word2Vec for word embedding\n",
    "- Apply a 'github' image mask to a WordCloud (within the negative/positive space; label each space with different colors)\n",
    "\n",
    "> **Questions:**  \n",
    "- What am I? Who are we?\n",
    "- Does the sentiment in a given languange vier more towards positive or negative?\n",
    "- How many graphs can we make this weekend??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Environment <a name=\"preparation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bring in the data from the prepare file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('data.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Going to see what the top portion looks like**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>readme_contents</th>\n",
       "      <th>repo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JavaScript</td>\n",
       "      <td>&lt;p align=\"center\"&gt;\\n  &lt;a href=\"https://getboot...</td>\n",
       "      <td>twbs/bootstrap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JavaScript</td>\n",
       "      <td># [React](https://reactjs.org/) &amp;middot; [![Gi...</td>\n",
       "      <td>facebook/react</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>This page is available as an easy-to-read webs...</td>\n",
       "      <td>EbookFoundation/free-programming-books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;div align=\"center\"&gt;\\n\\t&lt;img width=\"500\" heigh...</td>\n",
       "      <td>sindresorhus/awesome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>![Web Developer Roadmap - 2019](https://i.imgu...</td>\n",
       "      <td>kamranahmedse/developer-roadmap</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     language                                    readme_contents  \\\n",
       "0  JavaScript  <p align=\"center\">\\n  <a href=\"https://getboot...   \n",
       "1  JavaScript  # [React](https://reactjs.org/) &middot; [![Gi...   \n",
       "2        None  This page is available as an easy-to-read webs...   \n",
       "3        None  <div align=\"center\">\\n\\t<img width=\"500\" heigh...   \n",
       "4        None  ![Web Developer Roadmap - 2019](https://i.imgu...   \n",
       "\n",
       "                                     repo  \n",
       "0                          twbs/bootstrap  \n",
       "1                          facebook/react  \n",
       "2  EbookFoundation/free-programming-books  \n",
       "3                    sindresorhus/awesome  \n",
       "4         kamranahmedse/developer-roadmap  "
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many Languages does our CSV have?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JavaScript          43\n",
       "Python              22\n",
       "Go                   7\n",
       "C++                  6\n",
       "Java                 6\n",
       "CSS                  3\n",
       "Jupyter Notebook     2\n",
       "Ruby                 2\n",
       "Swift                2\n",
       "HTML                 2\n",
       "Rust                 1\n",
       "TypeScript           1\n",
       "C                    1\n",
       "PHP                  1\n",
       "Assembly             1\n",
       "Dart                 1\n",
       "C#                   1\n",
       "Name: language, dtype: int64"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.language.value_counts()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Funtions to clean our readme files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(text):\n",
    "    '''\n",
    "    Function that takes a string and normalized the text using unicodedata\n",
    "    '''\n",
    "    text = unicodedata.normalize('NFKD', text.lower())\\\n",
    "        .encode('ascii', 'ignore')\\\n",
    "        .decode('utf-8', 'ignore')\n",
    "    return re.sub(r\"[^a-z0-9'\\s]\", '', text)\n",
    "\n",
    "def lemmatize(text):\n",
    "    '''\n",
    "    Function that lemmatizes the string\n",
    "    '''\n",
    "    nlp = spacy.load('en', parse=True, tag=True, entity=True)\n",
    "    doc = nlp(text) # process the text with spacy\n",
    "    lemmas = [word.lemma_ for word in doc]\n",
    "    text_lemmatized = ' '.join(lemmas)\n",
    "    return re.sub(r\"\\s*(-PRON-|\\'s|\\')\", '', text_lemmatized)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    '''\n",
    "    Function to remove stopwords from the string\n",
    "    '''\n",
    "    tokenizer = ToktokTokenizer()\n",
    "    stopword_list = stopwords.words('english')\n",
    "    stopword_list.remove('no')\n",
    "    stopword_list.remove('not')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    filtered_tokens = [t for t in tokens if t not in stopword_list]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "def clean_readme(string):\n",
    "    '''\n",
    "    Function that pipes the funtions together\n",
    "    '''\n",
    "    return remove_stopwords(lemmatize(basic_clean(string)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply our funcitons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['readme_clean'] = df.readme_contents.apply(clean_readme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thoughts:**\n",
    ">- We no longer need the original readme contents, going to dump them\n",
    ">- We need to now group by the language\n",
    ">- We are going to start using WordCloud to visually see the frequencies of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='readme_contents', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>repo</th>\n",
       "      <th>readme_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JavaScript</td>\n",
       "      <td>twbs/bootstrap</td>\n",
       "      <td>p aligncenter hrefhttpsgetbootstrapcom img src...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JavaScript</td>\n",
       "      <td>facebook/react</td>\n",
       "      <td>reacthttpsreactjsorg middot github licensehttp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>EbookFoundation/free-programming-books</td>\n",
       "      <td>page available easytoread website httpsebookfo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>sindresorhus/awesome</td>\n",
       "      <td>div aligncenter &amp;#9; img width500 height350 sr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>kamranahmedse/developer-roadmap</td>\n",
       "      <td>web developer roadmap 2019httpsiimgurcoms5ccv9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     language                                    repo  \\\n",
       "0  JavaScript                          twbs/bootstrap   \n",
       "1  JavaScript                          facebook/react   \n",
       "2        None  EbookFoundation/free-programming-books   \n",
       "3        None                    sindresorhus/awesome   \n",
       "4        None         kamranahmedse/developer-roadmap   \n",
       "\n",
       "                                        readme_clean  \n",
       "0  p aligncenter hrefhttpsgetbootstrapcom img src...  \n",
       "1  reacthttpsreactjsorg middot github licensehttp...  \n",
       "2  page available easytoread website httpsebookfo...  \n",
       "3  div aligncenter &#9; img width500 height350 sr...  \n",
       "4  web developer roadmap 2019httpsiimgurcoms5ccv9...  "
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration  <a name=\"exploration\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thoughts:**\n",
    ">- Look at Top\\Bottom 10 words in each readme grouped by programming language\n",
    ">- Sentiment Analysis on each languange\n",
    ">- Figure out how to apply latent Dirichlet allocation\n",
    ">- Learn how to set up Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict(df, language):\n",
    "    '''\n",
    "    Function that creates a dataframe with words in the repo and their counts\n",
    "    '''\n",
    "    language_dict = {}\n",
    "    temp_df = df[df['language'] == str(language)]\n",
    "    \n",
    "    for i, row in temp_df.iterrows():\n",
    "        row_text = str(row['readme_clean'])\n",
    "        row_text_words = row_text.split(' ')\n",
    "        \n",
    "        for word in row_text_words:\n",
    "            if word in language_dict:\n",
    "                language_dict[word] += 1\n",
    "            else:\n",
    "                language_dict[word] = 1\n",
    "                \n",
    "    return pd.DataFrame.from_records(language_dict, index=[0]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grab any language and find the frequency**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "js_frequency = get_dict(df, 'JavaScript')\n",
    "python_frequency = get_dict(df, 'Python')\n",
    "go_frequency = get_dict(df, 'Go')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <td>2381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use</th>\n",
       "      <td>1668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>detail</th>\n",
       "      <td>1475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>js</th>\n",
       "      <td>1385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>const</th>\n",
       "      <td>1147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>php</th>\n",
       "      <td>931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>back</th>\n",
       "      <td>894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>return</th>\n",
       "      <td>848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mit</th>\n",
       "      <td>839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "source  2381\n",
       "use     1668\n",
       "detail  1475\n",
       "js      1385\n",
       "const   1147\n",
       "php      931\n",
       "back     894\n",
       "return   848\n",
       "mit      839\n",
       "1        743"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "js_frequency.sort_values([0], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Graphs** _(todo)_\n",
    ">\n",
    "- Stacked Bargraph\n",
    "- WordCloud\n",
    "- anything extra we see"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling <a name=\"modeling\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bring in multiple classification models**\n",
    "> _todo_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split\n",
    "> _todo_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize Conclusions <a name=\"summary\"></a>\n",
    "> _todo_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find different ways to improve model:\n",
    "> _todo_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
